# Template para Tests de Vapi
# Copia este archivo y modifícalo según tus necesidades

# ========================================
# PROVIDER CONFIGURATION
# ========================================

# Especifica el provider (obligatorio para Vapi)
provider: vapi

# Categoría del test - se usa para agrupar en test suites
# Tests con la misma categoría se ejecutan juntos
category: your-category-name  # Ej: loan-application, customer-service, error-handling

# Tags opcionales para filtrado y organización
tags: [p0, smoke]  # Ej: p0, p1, p2, smoke, regression, etc.

# ========================================
# TEST METADATA
# ========================================

name: "Nombre Descriptivo del Test"
description: "Descripción detallada de qué valida este test"

# ID del agente (usa variable de entorno o especifica directamente)
agent_id: "${ELEVENLABS_AGENT_ID}"

# ========================================
# VAPI-SPECIFIC CONFIGURATION
# ========================================

vapi:
  # ID del assistant de Vapi (opcional si está en .env)
  assistant_id: "${VAPI_ASSISTANT_ID}"

  # Número de veces que se ejecuta el test (1-5)
  # Útil para validar consistencia en respuestas
  attempts: 1

# ========================================
# SIMULATED USER
# ========================================

simulated_user:
  # Prompt que define el comportamiento del usuario simulado
  # Puede ser texto libre multilínea
  prompt: |
    You are a [customer/user role].

    Your scenario:
    - [Describe the situation]
    - [What do you want to accomplish]
    - [Any specific information to provide]

    Your behavior:
    - [How should you act]
    - [How should you respond]
    - [Any specific phrases to use]

    Use variables with {{variable_name}} syntax.

  # Primer mensaje del usuario
  first_message: "Hello, I need help with..."

  # Idioma de la conversación
  language: "en"  # en, es, fr, de, etc.

  # Temperatura (0.0-1.0) - controla la creatividad
  # 0.0-0.3: Respuestas precisas y consistentes
  # 0.4-0.7: Balance entre consistencia y naturalidad
  # 0.8-1.0: Respuestas variadas y creativas
  temperature: 0.5

  # Opcional: LLM específico
  # llm: "gpt-4o"

  # Opcional: Max tokens por respuesta
  # max_tokens: 150

# ========================================
# EVALUATION CRITERIA
# ========================================

evaluation_criteria:
  # Lista de criterios que se evalúan al final de la conversación
  # Cada criterio se convierte en una pregunta para el LLM evaluador

  - id: "criterion-1"
    name: "Nombre del Criterio"
    prompt: "Did the agent [accomplish specific goal]?"

  - id: "criterion-2"
    name: "Otro Criterio"
    prompt: "Was the [specific aspect] handled correctly?"

  # Tips para buenos criterios:
  # - Hacer preguntas específicas y medibles
  # - Usar "Did the agent..." format
  # - Enfocarse en un aspecto por criterio
  # - Incluir tanto aspectos funcionales como de UX

# ========================================
# DYNAMIC VARIABLES
# ========================================

# Variables que se inyectan en el script del usuario simulado
# Usar sintaxis {{variable_name}} en el prompt
dynamic_variables:
  customer_name: "John Doe"
  document_number: "1234567890"
  phone: "+1-555-0123"
  email: "john.doe@example.com"
  # Agrega todas las variables que necesites

# ========================================
# EXAMPLES BY USE CASE
# ========================================

# Happy Path Test:
# - category: happy-path
# - tags: [p0, smoke]
# - temperature: 0.3 (consistente)
# - Focus: Flujo exitoso completo

# Error Handling Test:
# - category: error-handling
# - tags: [p1, error]
# - temperature: 0.4-0.6
# - Focus: Cómo maneja datos inválidos o errores

# Edge Case Test:
# - category: edge-cases
# - tags: [p2, edge]
# - temperature: 0.5-0.7
# - Focus: Situaciones inusuales pero válidas

# Performance Test:
# - category: performance
# - tags: [p1, performance]
# - attempts: 5
# - Focus: Consistencia en múltiples ejecuciones

# ========================================
# OPTIONAL FIELDS
# ========================================

# Para tests persistentes (create mode):
# success_condition: "El agente debe..."
# success_examples:
#   - "Respuesta apropiada ejemplo 1"
# failure_examples:
#   - "Respuesta inapropiada ejemplo 1"

# Tool mocking (si el agente usa herramientas):
# tool_mock_config:
#   tool_name:
#     return_value: "Mocked response"
#     should_fail: false

# Conversación parcial previa (para tests de contexto):
# partial_conversation_history:
#   - role: "user"
#     message: "Previous message"
#   - role: "agent"
#     message: "Previous response"

# Límite de turnos (para conversaciones largas):
# new_turns_limit: 20
